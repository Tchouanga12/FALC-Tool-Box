# -*- coding: utf-8 -*-
"""recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17uF0F6_x6SL_Gio0xEkgqlzFQuuhqzlV
"""
##TO-DO just keep what is needed in the imports
import PyPDF2
import re
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.tokenize import TreebankWordTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from numpy.linalg import svd
from gensim.models import Word2Vec
import pickle





class preprocess:
  """
  NLP preprocessing by Engineers at ISEP

  """
  def __init__(self):
      self.sentence_token = list()
      self.total_token = 0

  def extract_text_pdf(self,pdf_directory):
      """
      This function collects text from a pdf and outputs a
      processed list of the text.

      input: pdf direcotry
      output: list of preprocessed text
      """
      pdf =  open(pdf_directory,'rb')
      pdf = PyPDF2.PdfFileReader(pdf)
      for i in range(pdf.numPages):
        print(i)
        pagepdf = pdf.getPage(i)
        _ , total = self.preprocess_text([self.lower_text(pagepdf.extractText())])
        self.total_token+=total
      print(f'In document {pdf_directory} a total of {pdf.numPages} page(s) was identified in the PDF and  {self.total_token} token(s) processed.')
      print()
      return self.sentence_token

  def lower_text(self,text):
      """
      removing spaces at the begining and end of string 
      and puts the text into small letters.

      input: text
      output: small case text
      """
      return  text.strip().lower()

  def get_sentences(self,text): 
      """
      Sentence tokenization or splitting function, breaks a bunch of 
      text into a set of sentence based on the dot (.) punctuation
      mark.

      input: text
      output: tokenized sentence
      """
      return sent_tokenize(self.lower_text(text),language='french')    

  def clean_text(self,text):
      """
      This function involves any cleaning process
      to be done on the text before it goes for continues
      preprocesing. This function takes no parameter.

      input: None
      output:splitted sentences based in part of speech tagging.
        """
      return self.get_sentences(text)

  def bind_num(self,matchobj):
      """
      Function to bind French numerical numbers

      input: regular expression object
      output: binded numerical number 
      """
      return ''.join(matchobj.group(0).split(' '))

  def preprocess_text(self,texts):
      """
      This function entails further preprocessing operations
      done on the text resulting in a set of tokens for each
      splitted sentence from text using spacy part of speech
      french tagging.

      input: list of sentence tokens
      output: list of tokens associated to each sentence
      """
      total = 0
      if type(texts) == str:
        print('ProcessErrro: Your input value should be in a form of a list try again')
      
      else: 
        try:
          count=1
          for text in texts:
            sentences = self.clean_text(text) ##Parser
            ##Lexical analyser and symmbol table creation per sentence
            for sentence in sentences:
              sentence = re.sub(",", ' ', sentence)
              sentence = re.sub("[0-9]+\s*.[0-9]+\s*.[0]+", self.dashrepl, sentence)
              sentence = re.sub(r"http\S+", "", sentence)
              sentence = re.sub("[A-Za-z0-9]*@[A-Za-z0-9]*.[A-Za-z]*", '', sentence)
              tokens = word_tokenize(sentence, language='french')
              tokens = [re.sub("[a-z]+[',’,']",'', token) for token in tokens] 
              tokens = [token for token in tokens if token != '\n'] ##Regular expression could also solve the problem
              tokens = [token for token in tokens if token not in ['*','.',',','«','(',')','»',"l'",'-',';','[',']','—',':','…','?','–','...','!','’',"'",'•','/','➢','&','|','=']] 
              tokens = [re.sub("[.,•]",'', token) for token in tokens]
              
              
              if '\n ' in tokens:
                  tokens.remove('\n ') ##Remove empty space symbols
              if len(tokens) != 0:
                self.sentence_token.append(tokens)
                total += len(tokens)
            count+=1 ##Count the number of available text
            print(f'The total number of {total} token(s) has been processed')
        except TypeError:
          print('Your data shoud be found inside a list')
        
        return self.sentence_token, total

class recognition:

   def __init__(self):

      self.token_to_complex = list()
      self.sentence_verbs = list()
      self.model = 0
      self.complex_words = 0
      self.lexique = pd.read_table('http://www.lexique.org/databases/Lexique383/Lexique383.tsv')
      self.lexique = self.lexique.groupby('ortho').sum()
      self.pca = PCA(n_components=1)


   def classifier1(self,model_path,tokens):

            token_features = self.lexique[self.lexique.index.isin(tokens)]
            token_features_num = token_features.select_dtypes(['int64','float64'])
            token_features_num = token_features_num.replace(-np.inf,0)
            input = self.pca.fit_transform(token_features_num)    
            # _sc = StandardScaler()
            # _pca = PCA(n_components = 1)
            # preprocess = Pipeline([
            #     ('std_scaler', _sc),
            #     ('pca', _pca),
            #     ('regressor', _model)
            # ])
            #          
            ##feature creation for model prediction
            ##load the model from disk
            filename = self.model ##path to machine learning model
            loaded_model = pickle.load(open(model_path, 'rb'))
            result = loaded_model.predict(input)

            ##Getting the list of complex words in the tokenized sentence
            token_features['class'] = result
            token_features['class'] = token_features['class'].replace(to_replace=[1,0], value=['simple', 'complex'])
            self.complex_words = token_features[token_features['class'] == 'complex'].index.to_list()

            return self.complex_words


   def complex_word_recognition(self,sentence_list,classifier1,word2vec,truth=''):
      """
      This function permits the extraction of complex words in 
      a sentence with the use of a classification model.

      input: tokenized set of sentences
      output: tokenized sentences with their associated complex words
      """
      self.token_to_complex = list()
      final = []
      result = []
      if type(sentence_list[0]) == str:
        print('TypeError:Your input value should be in a form of a list try again') ##Check data validity
      else:

        not_found = []

        self.model = Word2Vec.load(word2vec) ## load word2vec model

        for sentence in sentence_list: ## Get each sentence in the sentence list
            for word in sentence: ## Get each word in each sentence
                if word not in self.model.wv.index_to_key:
                  not_found.append(word) ## If sentence not found in vocabulary update rhe vocabulary
            if len(not_found) !=0:
              self.model.build_vocab([not_found], update=True)
              self.model.train([not_found],total_examples=self.model.corpus_count, epochs=self.model.epochs) #Then retrain the model
            
            complex_words = self.classifier1(classifier1,sentence)
            self.complex_words.remove(self.model.wv.doesnt_match(complex_words)) 
            comlex_word_features  = self.get_features(self.complex_words,word2vec)
            for word in self.complex_words:
                  cos_sim_avg = np.average(self.model.wv.cosine_similarities(self.model.wv[word],self.model.wv[sentence])) \
                  * self.lexique[self.lexique.index.isin([word])]['freqfilms2'].values ## Compute cosine similarity of each word with words in the vocabulary giving a specific value to differentiate betwwen complex and simple words.
                  
                  if cos_sim_avg < 1:
                    print(word, cos_sim_avg,self.lexique[self.lexique.index.isin([word])]['freqfilms2'].values)
                    final.append(word)
        result.append([' '.join(sentence),self.complex_words])

      return  result


   def tense_recognition(self):
      count = 1
      for tokens in self.tokenized_sentences: 
        ##Parse tokens
        doc = self.nlp(' '.join(tokens))
        sent = list(doc.sents)[0]
        ##Visualize parsing
        print(sent._.parse_string)

        #Extract verbs in text
        verbs = list()
        exp = re.compile('[(V]* ') ##Regular expression to extract all verbs
        for i in range(0,len(tokens)):
          word = list(doc.sents)[0][i]
          print('word',word)
          if len(list(doc.sents)[0][i]._.labels)!=0:
            if 'VN' in list(doc.sents)[0][i]._.labels and 'VINF' not in list(doc.sents)[0][i]._.parse_string:
              verbs.append(word)
          elif exp.match(list(doc.sents)[0][i]._.parse_string):
            verbs.append(word)

        ##get the word lemme and its tense value
        view = self.lexique[(self.lexique['ortho'].isin([str(i) for i in verbs])) & (self.lexique['cgram'] == 'VER')]

        ##Create output in the form of a list
        for i in range(0,len(view.ortho.values)):
          self.sentence_verbs.append({'sentence_tokens':tokens,'verb':view.ortho.values[i], 'lemme':view.lemme.values[i], 'tense':view.infover.values[i]})
        print(f'Completed sentence {count} and stored')
        print()##add some space
